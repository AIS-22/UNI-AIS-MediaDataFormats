\begin{filecontents}{shortbib.bib}
@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
\end{filecontents}

\documentclass{article}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage{biblatex}
\bibliography{shortbib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, % Set this to true to remove the border
    linkcolor=black, % Color for internal links
    citecolor=black, % Color for citations
    }
\usepackage{listings}
\usepackage{blindtext}
\usepackage{cleveref}
\usepackage{subcaption}
\geometry{margin=25mm}
\begin{document}

\section{Encoding Software}
\label{sec: encoder}

To encode the loss less images into the respective codec for the experiment, several encoder where used.
\subsection{AVIF, JPEG and WebP}
\subsubsection{AVIF}
AVIF is a format that utilizes the AV1 video codec. The AV1 codec employs block-based predictive coding. In this method, the frame is broken down into smaller units. Those are then analyzed to predict repetitive motion and color data based on blocks that have been processed earlier. Once the prediction is made, the redundant data is removed from the block. 
To convert the image into a sum of cosine functions of different frequencies the Discrete Cosine Transform (DCT) is used.

\subsubsection{JPEG}
JPEG uses the Discrete Cosine Transform (DCT) to convert an image into a sum of cosine functions of different frequencies. After the DCT, the coefficients are quantized to reduce their precision, and then entropy coded to compress the resulting data. The entropy coding method used in JPEG is Huffman coding.

\subsubsection{WebP}
In WebP a similar compression mechanism to that of the VP8 codec is used. This involves segmenting the frame into smaller units. Each of those units motion and color information is predicted based on previously processed blocks. This prediction process allows for the removal of redundant data from each block. The remaining data is then transformed using the Discrete Cosine Transform (DCT).

For those encoders the python package pillow-avif-plugin and pillow was used. The additional pillow plugin can be installed via pip install pillow-avif-plugin. To encode a lossless image, the image is opened and save with a specific quality as seen in listing \ref{avif_1}. The quality parameter ranges from 1(lowest) to 100(highest). It is important to set the specific file extension for the software to
know in which codec the image needs to be encoded.
\begin{lstlisting}[label={avif_1}, language=Python, caption=Encode AVIF\, JPEG and WebP]
image = Image.open(image_path)
image.save(outputPath, quality=q)
\end{lstlisting}

\subsection{BPG}
The Better Portable Graphics (BPG) codec is a digital image coding file format, developed by Fabrice Bellard. It’s built upon the intra-frame encoding of the High Efficiency Video Coding (HEVC) video compression standard.

BPG operates on a block-by-block basis to transform image data into a form that minimizes redundancy. It employs high-quality decimation and interpolation (using 10 tap Lanczos filter and 7 tap Lanczos filter respectively) to manage chroma samples in 4:2:2 and 4:2:0 formats.

To encode the images in BPG the Linux distribution from Fabrice Bellard's website (https://bellard.org/bpg/) was used. As seen in listing \ref{bpg_1} the output path is given after the -o argument and the specific quality after the -q argument. The quality parameter ranges from 51(lowest) to 0(highest). The last argument that needs no specific indicator is the original image.

\begin{lstlisting}[label={bpg_1}, language=Python, caption=Encode BPG]
os.system('bpgenc -o ' + outputPath + \
    ' -q ' + str(int(maxQ - q)) + \
    ' ' + image_path)
\end{lstlisting}

\subsection{HEIC}
The High Efficiency Image File (HEIF) container, which uses the High Efficiency Video Coding (HEVC) compression standard, is the basis for the HEIC codec. This codec is used for storing both images and videos. 
The transformation is applied to pixel blocks, the size of which can range from 4x4 to 32x32. Depending on the type of block and the prediction mode, the transformation can be either a discrete cosine transform (DCT) or a discrete sine transform (DST).
Depending on the profile and level of the HEVC standard, the entropy coding can be either a context-adaptive binary arithmetic coding (CABAC) or a context-adaptive variable-length coding (CAVLC).

Same as with the AVIF codec HEIC is also supported in a python package. For this codec the package pillow\_heif which can be installed with pip pillow-heif. The quality parameter ranges from 1(lowest) to 100(highest).
How the image can be encoded in HEIC is shown in listing \ref{heic_1}.

\begin{lstlisting}[label={heic_1}, language=Python, caption=Encode HEIC]
image = pillow_heif.from_pillow(Image.open(image_path))
image.save(outputPath, quality=q)
\end{lstlisting}

\subsection{JPEG2000}
In JPEG2000, the transformation process employs a discrete wavelet transform (DWT). The original image is divided into rectangular tiles that do not overlap. These tiles are then broken down into different levels of decomposition using DWT. Each level of decomposition contains a number of subbands, which are made up of coefficients that represent the characteristics of the original tile component’s horizontal and vertical spatial frequencies.
For entropy coding JPEG2000 uses Embedded Block Coding with Optimized Truncation (EBCOT). EBCOT is a context-based binary arithmetic encoder that consists of a Context Extractor (CE) and an Arithmetic Coder (AC).

For the JPEG standards JPEG itself provides software to encode images. In the experiment the libopenjp2 was used. This library is in the Linux package manager and can be installed with apt-get install libopenjp2-tools. The quality parameter which ranges from 1(lowest) to 1000(highest) is given to the software with the -r parameter. An example how this can be done in Python is shown in listing \ref{jpeg2k_1}.
\begin{lstlisting}[label={jpeg2k_1}, language=Python, caption=Encode JPEG 2000]
subprocess.call('opj_compress -o ' + outputPath + \
                ' -r ' + str(int(maxQ - q)) + \
                ' -i ' + image_path,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                shell=True)
\end{lstlisting}

\subsection{JXL}
JXL also uses the same basic technologies as JPEG2000, but with some improvements and innovations, such as learned lifting-based DWT and learned tree-based entropy coding.

For JPEG XL the JPEG organization also provides a package which can be installed in Linux. For that encoder the libjxl (apt install libjxl-devtools) was used. The quality parameter ranges from 1(lowest) to 100 (highest) and an example to encode a image is shown in \ref{jxl_1}
\begin{lstlisting}[label={jxl_1}, language=Python, caption=Encode JPEG XL]
subprocess.call(['cjxl', image_path, outputPath,
                 '--quiet', '-q', str(q)])
\end{lstlisting}
\subsection{JXR}
JXR uses a unique method that breaks down an image into smaller, individual rectangular tile areas. This is different from JPEG, which uses a single transformation stage. Instead, JXR applies its 4x4 core transform in a two-level hierarchical manner within 16x16 macroblock regions. This approach gives the transform a wavelet-like multi-resolution hierarchy, which enhances its compression capability.

When it comes to entropy coding, JXR’s process is more adaptable and intricate than JPEG’s. It includes a DC and AC coefficient prediction scheme, adaptive coefficient reordering (as opposed to JPEG’s fixed zigzag ordering), and a type of adaptive Huffman coding for the coefficients themselves. This entropy coding process enables efficient representation of the image data, which contributes to the overall compression performance.

Additionaly the Photo Overlap Transform (POT) is a key operation in JPEG XR encoding that reduces blocking artefacts and improves compression efficiency. There are three ways of setting the POT in JPEG XR, depending on the image format and the compression mode:

\textbf{No POT:} This is the simplest option, where no POT is applied to the image blocks. This is suitable for lossless compression or images with low bit depth (8 bits or less per channel).

\textbf{Half POT:} This option applies POT to half of the image blocks, alternating between odd and even rows and columns. This reduces the computation complexity and memory requirement of the POT, while still providing some benefits of overlap processing. This is suitable for lossy compression or images with medium bit depth (9 to 15 bits per channel).

\textbf{Full POT:} This option applies POT to all of the image blocks, creating a full overlap between adjacent blocks. This maximizes the quality and compression performance of the POT, but also increases the computation complexity and memory requirement. This is suitable for high-quality compression or images with high bit depth (16 bits or more per channel).

For JXR the package libjxr-devtools was used. This includes a specific encoder and decoder app. The quality ranges from 0(lowest) to 1(highest). Here floating point numbers need to be used for quality.
An example how to encode a image with jxr is shown in \ref{jxr_1}. In addition to the quality parameter jxr also support quantization, although this results in very good compression rate, the image quality is affected drastically.
\begin{lstlisting}[label={jxr_1}, language=Python, caption=Encode BPG]
os.system('JxrEncApp -q ' + q_str + \
            ' -o ' + output_path + \
            ' -i ' + tif_path)
\end{lstlisting}

\newpage
\section{Achieving Fixed Filesize Compression Through Binary Search Algorithm}
\label{sec: filesize}

The primary objective was to attain a consistent and fixed filesize for the encoder discussed in chapter \ref{sec: encoder}. To achieve this goal efficiently, we devised a binary search algorithm, strategically designed to conserve computational resources and time. This algorithm, when coupled with the encoders enabled us to successfully compress the original images while maintaining a specified filesize. The results are graphically represented in image \ref{fig: fsize_comparison}, which displays boxplots illustrating the distribution of the encoder performances for targeting file size 32kB.

\begin{figure}[h!]
	\centering
	\resizebox{\textwidth}{!}{\input{../Plots/filesize_to_target.pgf}}
	\caption{Archived filesizes for used encoder}
	\label{fig: fsize_comparison}
\end{figure}

\noindent
To validate the performance of the utilized encoder, we carried out assessments involving two key metrics: Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index (SSIM). Both metrics got calculated as mean values over 5 images. These evaluations were displayed in plot \ref{fig: comparison} to generate graphical representations illustrating the correlation between these metrics and the resultant compression rates. To ensure visual clarity within the plots, we employed the JPEGXR compression algorithm with its default overlapping parameter set to 1.

\begin{figure}[h!]
    \centering
      \begin{subfigure}{0.49\textwidth}
        \resizebox{\textwidth}{!}{\input{../Plots/psnr.pgf}}
          \caption{PSNR comparison of the used codecs}
          \label{fig: psnr_comparison}
      \end{subfigure}
      \hfill
      \begin{subfigure}{0.49\textwidth}
        \resizebox{\textwidth}{!}{\input{../Plots/ssim.pgf}}
          \caption{SSIM comparison of the used codecs}
          \label{fig:CI}
      \end{subfigure}
\caption{
\label{fig: comparison}%
Comparison of PSNR and SSIM}
\end{figure}

\begin{figure}[h!]
    \centering
      \begin{subfigure}{0.49\textwidth}
        \resizebox{\textwidth}{!}{\input{../Plots/psnr_adapted.pgf}}
          \caption{PSNR comparison of the used codecs}
          \label{fig: psnr_comparison}
      \end{subfigure}
      \hfill
      \begin{subfigure}{0.49\textwidth}
        \resizebox{\textwidth}{!}{\input{../Plots/ssim_adapted.pgf}}
          \caption{SSIM comparison of the used codecs}
          \label{fig:CI}
      \end{subfigure}
\caption{
\label{fig: comparison}%
Comparison of PSNR and SSIM}
\end{figure}

\noindent
As anticipated, the results presented in plot \ref{fig: comparison} affirm that the plain JPEG encoder consistently yields in reduced-quality images when operating at lower compression rates.
The driving motivation behind this attempt was to intentionally introduce diverse encoding artifacts into the compressed images for later train a model and classify the used image encoder.
\newpage

\section{RESNET explanation}
Residual Networks are deep learning models, that where developed by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. This networks enable easier training of deep learning models. For an explicit explanation on how a ResNet works see \cite{he2015deep}.

\subsection{Training without transfer learning}

The idea behind training the network from scratch is that the network would be more tailored to the given task since it has no previous information stored in the network. This contrasts with transfer learning since there prior information is present. As a model the ResNet18 architecture was used. This architecture was chosen since it is a good compromise between network size and performance.
\newline

\noindent
\textbf{Dataset Details:}

\noindent
To train the network the DIV2K training and validation dataset was used. The dataset contains 800 train images and 100 test images. Since the images had different dimensions, our first step was to crop all images to the same size in our case 512x512. Afterwards we compressed the images to specified file sizes as described in \ref{sec: filesize}. With the primary objectice beeing to recignize the different compression algorithms the output of the network should represent the different encoding schemes. This leads to 10 classes in each file size category.

\noindent
\textbf{Training Details:}

\noindent
For the self-trained network, we used an Adam Optimizer with a learning rate of 1e-4 and a weight decay of 1e-4. As a loss function Cross entropy was used. To be able to match the output of the network to the 10 classes used we replaced the final fully connected layer in the Resnet architecture with one that outputs the 10 codec classes.

\noindent
In the first iteration we trained the network on the original cropped 800 images per class. This leads to 8000 images available for the network to train. With that amount there was little progress during the first epochs. To be able to train on a larger dataset we then cropped the corners of each image to 512x512 to get 5x the number of usable images. This lead to 4000 images per class for the network to train.

\noindent
After 10 epochs of training, we achieved a validation accuracy of 57,6 percent avaged over the 5 training filesizes (5, 10, 17, 25 and 32). For 10 epochs this is lower than the results we got for the transfer learning approach. However, if one would increase the number of epochs, we expect to reach the same levels of accuracy since the only difference is that the pretrained network has a preferable starting position compared to the one without transfer learning. This can be shown by changing the starting seed to something different leading to only 10 percent accuracy after 10 epochs.

\subsection{Transferlearning}
To be able to compare the results of the self-trained model with a pretrained model, the ResNet18 architecture was used again. The pretrained model was already trained on 1,2 million images to give an preferable starting position for training. To adjust this pretrained model for the requirements of the experiment, the output layers where reconfigured to represent the 10 different codecs in which the images where encoded.
In addition the model uses just 224x224 images as inputs, thus the encoded images where center cropped to fit that size.



\noindent
Then the model was trained over 10 epochs on 4000 test images. The learning rate was set to 0.001 with a momentum of 0.9 due to the model being already trained on a huge data set and in the experiment just the adjustment for the codec artifacts should be added. For the whole implementation see appendix.

\section{Results}
\label{Results}
A mixed model was developed by training the model with images of filesizes 5, 10, 17, 25 and 32. This resulted in 4500 images (512x512) for each codec. The considered dataset has only 900 images. To compare the filesize models with the mixed model, the filesize models need the same amount of input images as the mixed model. Therefore, each image of the dataset was cropped into five images to get an amount of 4500 images for the training and testing.

\subsection{Evaluation of ResNet-18 Performance Without Transfer Learning} 
Prior to the implementation of transfer learning methodologies, an analysis was conducted to ascertain whether the ResNet-18 architecture could independently identify various image compression algorithms. The entire dataset of one specific compression size, was processed through the model, and the outputs were L2 normalized. This resulted in 1000 dimension vectors for each image.

To make sense of these results, we used Principal Component Analysis (PCA) on all the vectors and plotted them on a scatter plot as it can be seen in Figure \ref{fig: no_transfer}. The scatter plot reveals that for images compressed to 10kB, there is no discernible clustering pattern specific to a single compression algorithm. However, it is observed that certain clusters exist where multiple compression algorithms are in close proximity to each other. This observation aligns with the inherent capability of the original network to classify images. Consequently, the images utilized across different compression algorithms form these compact clusters.

\begin{figure}[h!]
	\centering
	\resizebox{\textwidth}{!}{\input{../Plots/plot_scatter_without_transfer_10.pgf}}
    \caption{Archived filesizes for used encoder}
	\label{fig: no_transfer}
\end{figure}

\section{Performance}
For the classification each model was evaluated with each filesize. The result is represented in the figure \ref{fig: acc_comparison}.

\begin{figure}[h!]
	\centering
	\resizebox{\textwidth}{!}{\input{../Plots/accuracy/accuracy_comparison.pgf}}
	\caption{Accuracy Comparison}
	\label{fig: acc_comparison}
\end{figure}

\begin{figure}[h!]
	\centering
	\resizebox{\textwidth}{!}{\input{../Plots/accuracy/accuracy_comparison_filtered.pgf}}
	\caption{Accuracy Comparison}
	\label{fig: acc_comparison}
\end{figure}

\begin{figure}[h!]
	\centering
	\resizebox{\textwidth}{!}{\input{../Plots/accuracy/accuracy_comparison_withoutMixed.pgf}}
	\caption{Accuracy Comparison}
	\label{fig: acc_comparison}
\end{figure}

\noindent
The self trained model was trained with the filesizes 5, 10, 17, 25 and 32 on all 10 codecs. This leads to 4000 images for each codec per epoch. As one can see in figure \ref{fig: acc_comparison} the self trained model has its best performances at filesizes 10, 17 and 25 ranging from 62 to 65 percent accuracy.

\noindent
The models trained with the filesizes 60, 75 and 100 have a low accuracy for small filesizes and increase for bigger filesizes. Small filesize images have more artifacts, which are not taken into account in this models. For that reason the models of 60, 75 and 100 filesize could only be used to classify codecs with the same filesizes.

\noindent
The smaller filesize models have their peaks around the filesizes they were trained on. However, for the images with less than 40kB this models have an accuracy of about 95 percent. For the bigger filesizes the accuracy is decreasing. The bigger filesizes have less artifacts in the images and also many codecs get the maximum quality after 60kB, which explains the decrease of accuracy for this models.

\noindent
An assumtion was that the mixed model would lead to the best result because of considering different filesizes for the model. As the figure \ref{fig: acc_comparison} shows, almost every small filesize model has a better performance than the mixed model.

\noindent
For classifying images with the given filesizes the model trained with 40kB would lead to the best result. However, for smaller filesize images (\textless 50kB) the model trained with 32kB images would be the right choice because of the bigger accuracy for 5kB images.

\section{Loss function}

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.7\textwidth}
        \centering
        \resizebox{\textwidth}{!}{\input{../Plots/loss_comparison/loss_comparison_fs_10_model.pgf}}
        \caption{Loss of model with filesize 10}
        \label{fig:loss_comparison_filesize_10}
    \end{minipage}\hfill
    \begin{minipage}{0.7\textwidth}
        \centering
        \resizebox{\textwidth}{!}{\input{../Plots/loss_comparison/loss_comparison_fs_mixed_model.pgf}}
        \caption{Loss of mixed model}
        \label{fig:loss_comparison_filesize_mixed}
    \end{minipage}
\end{figure}

\noindent
In the figure \ref{fig:loss_comparison_filesize_10} and \ref{fig:loss_comparison_filesize_mixed} the loss over ten epochs is represented. After 10 epochs the loss is already minimal, therefore further training would not make a significant change. The other nine models show a similar behavior as figure \ref{fig:loss_comparison_filesize_10} shows.


\printbibliography
\end{document}
